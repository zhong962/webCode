<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">  <!-- IE的设置 -->
    <meta name="viewport" content="width=device-width, inital-scale=1.0"> <!-- 响应式设置 -->
    <title>Seven common mistakes that should be avoided in machine learning practice</title>
	<script src="http://www.jq22.com/jquery/jquery-1.10.2.js"></script>
	<link rel="stylesheet" type="text/css" href="../css/public.css"/>
	<link rel="stylesheet" type="text/css" href="../css/style2.css"/>


    <link rel="stylesheet" href="./css/normalize.css"> <!-- CSS资源引入 -->
    <link rel="stylesheet" href="./css/page.css"> <!-- CSS资源引入 -->
    <link rel="shortcut icon" href="./images/logo.png" > <!-- 图标 -->
  </head>
  <body>

  <div class="top"></div>
	<div class="pr navpr">
		<div class="nav pa">
		<div class="w">
			<img class="fl mt9" src="../img/nav-1.png"/>
      <ul class="fl mt16 ml40">
					<li><a href="../index2.html">Index</a></li>
					<li><a href="../news/index2.html">News</a></li>
					<li><a href="../aichutan/index2.html">Explorer</a></li>
					<li><a href="../tushuoai/index2.html">Picture</a></li>
					<li><a href="../zhinengshibie/index2.html">Identification</a></li>
					<li><a href="../jiqikongzhi/index2.html">Machine</a></li>
					<li><a href="../linjunrenwu/index2.html">Leader</a></li>
					<li><a href="../guidianai/index2.html">Campus AI</a></li>
					<li><a href="../xgzz/index2.html">Book</a></li>
					<li><a href="../about/index2.html">About</a></li>
					<div class="clear"></div>
				</ul>
			<div class="nav-btn fr mt17">
				<a class="mr12" href="page123.html">中文</a>|
				<a class="ml10" href="page124.html">English</a>			</div>
			<div class="clear"></div>
		</div>
		<script>
	$(function(){
		var a = $('.nav'),
			b =a.offset();
		$(document).on('scroll',function(){
			var	c = $(document).scrollTop();
			if(b.top<=c){
				a.css({'position':'fixed','top':'0px'})
				}else{
					a.css({'position':'absolute','top':'0px'})
					}
			})
		})
	</script>
	</div>
	</div>


    <div class="bj">

      <!-- 中间部分的内容 -->
      <div class="bj-zhong">
        <h1 class="title">Seven common mistakes that should be avoided in machine learning practice</h1>
        <div class="two">
          <p>In the field of machine learning, there are dozens of solutions for each given modeling problem, and each model has different assumptions that are difficult to determine whether it is reasonable or not. In this case, most practitioners tend to choose modeling algorithms that they are familiar with. The authors argue that the assumptions of model algorithms do not necessarily apply to the data at hand; in the pursuit of optimal model performance, it is important to select model algorithms that are suitable for data sets (especially "large data").</p>
          <h3>Statistical modeling is very similar to engineering development.</h3>
          <p>In engineering development, there are many ways to build a key-value storage system, each design has different assumptions for use patterns. In statistical modeling, there are also many algorithms to construct a classifier, each algorithm also has a hypothesis set for data.</p>
          <p>When dealing with a small amount of data, because the experimental cost is very low, we try as many algorithms as possible to select the best algorithm. But when it comes to "big data," analyzing the data ahead of time, and then designing the corresponding "pipeline" model (preprocessing, modeling, optimization, evaluation, and production) is twice the result with half the effort.</p>
          <p>As mentioned in my previous article, there are dozens of solutions to each given modeling problem. Each model assumes different assumptions, and it is difficult for us to discern which assumptions are reasonable. In the industry, most practitioners tend to choose the modeling algorithm they are familiar with, rather than the one that fits the data set best. In this article, I will share some common misunderstandings (to avoid). In future articles, we will introduce some best practices.</p>
          <h3>1. assume the default loss function.</h3>
          <p>Many practitioners like to use default loss functions (such as square error) to train and select the optimal model. In fact, the default loss function rarely meets our business needs. Take fraud detection. When we detect fraud transactions, our business needs are to minimize the losses caused by fraud. However, the default loss function of the existing two classifier is equally harmful to false positives and omissions. For our business needs, the loss function not only punishes the missed report more than the missed report, but also punishes the missed report in proportion to the amount of fraud. Moreover, the training dataset of fraud detection is often extremely unbalanced. In this case, the loss function should be biased towards the care of rare classes (such as ascending / descending sampling).</p>
          <h3>2. dealing with nonlinear problems with ordinary linear models.</h3>
          <p>When it comes to building a binary classifier, many people immediately think of logical regression because it's simple. However, they forget that logistic regression is a linear model and that the cross-features of nonlinear factors need to be handcoded. Back to the example of fraud detection, in order to achieve good model effect, we need to introduce "billing address = delivery address &amp; transaction amount.</p>
          <h3>3. ignore outliers.</h3>
          <p>The outliers are interesting. According to the context, they need to be specially treated or should be completely ignored. Take income forecasts. If abnormal peaks of income are observed, we may have to pay more attention to them and analyze what causes them. But if the outliers are caused by mechanical errors, measurement errors, or any other non-generalized factor, we'd better filter them out before preparing the training data.</p>
          <p>Some model algorithms are very sensitive to outliers. For example, AdaBoost will pay more attention to them and give a considerable weight value. On the contrary, decision trees simply treat them as erroneous classifications. If the data set contains a considerable number of outliers, it is important to use a robust modeling algorithm with outliers or to filter out outliers directly.</p>
          <h3>4. the high variance model is used when the sample size is far less than the characteristic number.</h3>
          <p>SVM is one of the most popular modeling algorithms, one of its powerful functions is to use different kernel functions to fit the model. The SVM kernel is considered to be a way to spontaneously combine existing features to form a higher dimensional feature space. Because the cost of this powerful feature is almost negligible, most people default to kernel functions when training SVM models.</p>
          <h3>5. do not do L1/L2 normalization without standardization.</h3>
          <p>Regularization using L1 or L2 is a commonly used method of linear regression or logical regression penalty weight system. However, many people are not aware of the importance of standardization when using these regularization methods.</p>
          <p>Back to fraud detection, imagine a linear regression model that treats transaction volume as a feature. Without regularization, when the amount of the transaction is in dollars, the fitting coefficient will be 100 times that of the dollar. At the same time, because L1 / L2 regularization imposes heavier penalties on items with large coefficient values, the dollar as a unit of transaction amount will be more punished in this dimension. Therefore, regularization is not equal. It often punishes features at a smaller scale. To alleviate this problem, it is necessary to standardize all features in the pretreatment process so that they are in an equal position.</p>
          <h3>6. do not consider linear correlation, use linear models.</h3>
          <p>Suppose we construct a linear model containing two variables of X1 and X2, and the real model is Y = X1 + X2. Ideally, if the data contain only a small number of noise points, the linear regression model can restore the real model. However, if there is a linear correlation between X1 and X2, it is equally good for most optimization algorithms, whether Y = 2 * X1, Y = 3 * X1 - X2 or Y = 100 * X1 - 99 * X2. Although this problem does not cause our prediction bias, it seems that it does not matter. However, it makes the problem sick, because the weight of coefficients can not be explained.</p>
          <h3>7.the absolute value of the coefficients of a linear model or a logistic regression model is interpreted as characteristic importance.</h3>
          <p>Because many existing linear regression methods return the p value of each coefficient, many people think that the greater the absolute value of the coefficient, the corresponding characteristics play a greater role. This is not true because (1) scaling the variable changes the absolute value of the coefficient; (2) if the feature is linearly related, its coefficient can be transferred from one-dimensional feature to another. In addition, the more feature dimensions a data set contains, the more likely it is to be linearly correlated between features, and the less reliable it is to use coefficients to explain the importance of features.</p>
          <p>These are 7 common mistakes in machine learning and practice. This list is incomplete, it just inspires the reader to think, and the assumptions of the model algorithm do not necessarily apply to the data at hand. In the pursuit of optimal model performance, it is important to choose the model algorithm that fits the data, not the one you are most familiar with.</p>
        </div>

      </div>
      <!-- 中间部分的内容end -->

    </div>
<iframe src="../bottom2.html" width="100%" height="200px" scrolling="No"  noresize="noresize" frameborder="0" id="bottomFrame"></iframe>
  </body>
</html>
